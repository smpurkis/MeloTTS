{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/smpurkis/MeloTTS.git\n",
    "!pip install -e .\n",
    "!pip install wandb\n",
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "import shutil\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "from melo.api import TTS\n",
    "from melo.models import Generator\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speed is adjustable\n",
    "speed = 1.0\n",
    "\n",
    "# CPU is sufficient for real-time inference.\n",
    "# You can set it manually to 'cpu' or 'cuda' or 'cuda:0' or 'mps'\n",
    "device_str = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float32\n",
    "\n",
    "# English\n",
    "# text = \"Did you ever hear a folk tale about a giant turtle? It's a story about a turtle that carries the world on its back.\"\n",
    "# texts = [text]\n",
    "texts = Path(\"bible-web.txt\").read_text().split(\"\\n\")\n",
    "\n",
    "# Load the model\n",
    "s = time()\n",
    "model = TTS(language=\"EN\", device=device_str, use_onnx=False)\n",
    "print(f\"Loaded model in {time() - s:.2f}s\")\n",
    "speaker_ids = model.hps.data.spk2id\n",
    "output_path = \"/tmp/en-default.wav\"\n",
    "model.model.dec_training = []\n",
    "# model.tts_to_file(\"blah\", speaker_ids[\"EN-US\"], output_path, speed=speed)\n",
    "\n",
    "\n",
    "generator_params = dict(\n",
    "    initial_channel=192,  # 192\n",
    "    resblock=\"1\",  # 1\n",
    "    resblock_kernel_sizes=[\n",
    "        5,\n",
    "        11,\n",
    "        #    3\n",
    "    ],  # [3, 7, 11]\n",
    "    resblock_dilation_sizes=[\n",
    "        [1, 3, 5],\n",
    "        [1, 3, 5],\n",
    "        # [1, 3, 5],\n",
    "    ],  # [[1, 3, 5], [1, 3, 5], [1, 3, 5]]\n",
    "    upsample_rates=[2, 4, 4, 4, 4],  # [8, 8, 2, 2, 2]\n",
    "    upsample_initial_channel=512,  # 512\n",
    "    upsample_kernel_sizes=[4, 8, 16, 4, 4],  # [16, 16, 8, 2, 2]\n",
    "    gin_channels=256,  # 256\n",
    ")\n",
    "\n",
    "distilled_generator = Generator(**generator_params).eval()\n",
    "\n",
    "total_params_dec = sum(p.numel() for p in model.model.dec.parameters())\n",
    "total_params_dec_distilled = sum(p.numel() for p in distilled_generator.parameters())\n",
    "print(f\"Total params in dec: {total_params_dec}\")\n",
    "print(f\"Total params in dec distilled: {total_params_dec_distilled}\")\n",
    "print(f\"Ratio: {total_params_dec_distilled / total_params_dec:.2f}\")\n",
    "\n",
    "# out_distilled = distilled_generator(\n",
    "#     torch.randn([1, 192, 299]), torch.randn([1, 256, 1])\n",
    "# )\n",
    "s = time()\n",
    "out_distilled = distilled_generator(\n",
    "    torch.randn([1, 192, 299]), torch.randn([1, 256, 1])\n",
    ")\n",
    "total_time_distilled = time() - s\n",
    "\n",
    "model.model.dec = model.model.dec.eval()\n",
    "s = time()\n",
    "out = model.model.dec(torch.randn([1, 192, 299]), torch.randn([1, 256, 1]))\n",
    "total_time = time() - s\n",
    "\n",
    "print(f\"Elapsed time distilled: {total_time_distilled:.2f}s\")\n",
    "print(f\"Elapsed time: {total_time:.2f}s\")\n",
    "print(f\"Time ratio: {total_time_distilled / total_time:.2f}\")\n",
    "assert out.shape == out_distilled.shape, f\"{out.shape} != {out_distilled.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# use the weights of the last resblock of the original model on\n",
    "# the distilled model\n",
    "# distilled_generator.resblocks[-1].load_state_dict(\n",
    "#     model.model.dec.resblocks[-1].state_dict()\n",
    "# )\n",
    "\n",
    "# training\n",
    "\n",
    "# split the data into training and evaluation\n",
    "# np random seed\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(texts)\n",
    "\n",
    "evaluation_split = 0.01\n",
    "\n",
    "training_texts = texts[: int(len(texts) * (1 - evaluation_split))]\n",
    "evaluation_texts = texts[int(len(texts) * (1 - evaluation_split) + 1) :]\n",
    "\n",
    "training = True\n",
    "\n",
    "lr = 1e-3\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "steps = len(training_texts) // batch_size\n",
    "eval_steps = len(evaluation_texts) // batch_size\n",
    "\n",
    "distilled_generator.train().to(device)\n",
    "optimizer = torch.optim.Adam(distilled_generator.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2 * steps, gamma=0.1)\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"MeloTTS decoder-generator distillation\",\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": lr,\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"evaluation_split\": evaluation_split,\n",
    "        \"speed\": speed,\n",
    "        \"model_hyperparameters\": generator_params,\n",
    "        \"model_size_ratio\": total_params_dec_distilled / total_params_dec,\n",
    "        \"model_time_ratio\": total_time_distilled / total_time,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_evaluation_loss = 0\n",
    "\n",
    "    eval_step_counter = 0\n",
    "\n",
    "    for step in tqdm(range(steps), desc=\"Steps\"):\n",
    "        training = step % 10 != 0\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Step {step + 1}/{steps}\")\n",
    "\n",
    "        batch = []\n",
    "\n",
    "        model.model.dec_training = []\n",
    "\n",
    "        # generate the batch of data\n",
    "        for i in tqdm(\n",
    "            range(batch_size),\n",
    "            desc=f\"Batch generate {'training' if training else 'evaluation'}\",\n",
    "            leave=False,\n",
    "        ):\n",
    "            if training:\n",
    "                text = training_texts[step * batch_size + i]\n",
    "            else:\n",
    "                if eval_steps == eval_step_counter:\n",
    "                    eval_step_counter = 0\n",
    "                text = evaluation_texts[eval_step_counter * batch_size + i]\n",
    "                eval_step_counter += 1\n",
    "\n",
    "            model.model.dec_training.append({\"text\": text})\n",
    "            model.tts_to_file(\n",
    "                text, speaker_ids[\"EN-Default\"], output_path, speed=speed, quiet=True\n",
    "            )\n",
    "\n",
    "            # load batch of data to train on\n",
    "            point = model.model.dec_training.pop(0)\n",
    "            x_in = point[\"x_in\"].type(dtype).to(device)\n",
    "            g_in = point[\"g_in\"].type(dtype).to(device)\n",
    "            o_out = point[\"o_out\"].type(dtype).to(device)\n",
    "            batch.append((x_in, g_in, o_out))\n",
    "\n",
    "        # train on batch\n",
    "        if training:\n",
    "            distilled_generator.train()\n",
    "        else:\n",
    "            distilled_generator.eval()\n",
    "\n",
    "        # run the whole batch through then run backwards and optimize\n",
    "        batch_loss = 0\n",
    "        if training:\n",
    "            optimizer.zero_grad()  # zero the gradient buffers\n",
    "\n",
    "        for x_in, g_in, o_out in tqdm(\n",
    "            batch, desc=f\"Batch {'training' if training else 'evaluation'}\", leave=False\n",
    "        ):\n",
    "            output = distilled_generator(x_in, g_in)  # forward pass\n",
    "            loss = F.mse_loss(output, o_out)\n",
    "            if training:\n",
    "                loss.backward()  # backpropagation\n",
    "            batch_loss += loss.item()\n",
    "        batch_loss /= len(batch)\n",
    "        if training:\n",
    "            epoch_loss += batch_loss\n",
    "        else:\n",
    "            epoch_evaluation_loss += batch_loss\n",
    "\n",
    "        if training:\n",
    "            optimizer.step()  # Does the update\n",
    "\n",
    "        print(\n",
    "            f\"{'Training' if training else 'Evaluation'} Batch Loss: {batch_loss / len(batch)}\"\n",
    "        )\n",
    "\n",
    "        # wandb log epoch, step, evaluation/training losses\n",
    "        if training:\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"step\": epoch * steps + step,\n",
    "                    \"loss\": epoch_loss,\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"step\": epoch * steps + step,\n",
    "                    \"evaluation_loss\": epoch_evaluation_loss,\n",
    "                }\n",
    "            )\n",
    "    # wandb log epoch, step, evaluation/training losses\n",
    "    if training:\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"loss\": batch_loss,\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"evaluation_loss\": batch_loss,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    Path(\"dec_distill/checkpoints\").mkdir(exist_ok=True)\n",
    "\n",
    "    # save model\n",
    "    print(\"Saving model...\")\n",
    "    torch.save(\n",
    "        distilled_generator.state_dict(),\n",
    "        f\"dec_distill/checkpoints/model_{epoch}_loss_{batch_loss}.pt\",\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
